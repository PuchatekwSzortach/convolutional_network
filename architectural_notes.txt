

Model.fit:

    Shuffle data
    For each batch:

       - For each layer from start to end:

            Call train_forward() - does computations as in forward, but caches inputs and outputs

        - For each layer from end to start:

            Call train_backward() - performs backpropagation and applies results


Ok, what kind of tests could I write? What would I want to test?

Input layer - that shape is legal, that when compiling it sets output shape correctly and that during forward and train_forward
it checks for correct input and sets output

Flatten layer - that sets output_shape correctly at construction, that at compile accepts only shapes that can be squeezed to 2D,
    that during forward and train_forward outputs correct stuff

Softmax - that checks for legal input shape, that computes correct output, that computes correct loss,
    that compute correct loss derivative, that updates itself correctly, that passes correct loss to previous layer

Convolution2D - that creates correct kernels,
    that computes correct convolutions, that computes correct derivatives, that updates itself correctly


Ok, what do we want to test with Model:
- test model calls proper functions for fit

Ok, now how will model.fit(x) work?

    Shuffle data

    For batch in data:

        x = batch

        for layer in self.layers:

            x = layer.train_forward(x)

        gradients = 1

        for layer in reversed(self.layers):

            gradients = layer.train_backward(gradients, learning_rate)

I guess :P

Ok, so what components are needed for backpropagation?

Our fit will have fit_batch().
fit_batch() would do:
- model would calculate loss
- pass that loss to softmax
- softmax would calculate gradiens to its preactivation
- flatten would calculate its gradients
- input would have a no op there I guess
- convolution 2D would do its gradients

start with model.get_loss()


Hmm, so I know how to compute error gradients across Softmax layer.
But - to do that it needs:
- predictions
- correct labels
Predictions it can cache in train_forward(), but what about correct labels? How do they get in there?
I need train_backward(correct_labels)

Convolution backpropagation:


Not yet checked cases:

Ok, tests we need for computing (and passing!) dC/da(l-1):
- all the existing tests - we can add gradients checks to them
- a larger case - say 3x3 kernel and 4x4 result, so 6x6 previous layer... I guess



Further tests needed:
- pass gradients from convolutional layer to upstream layer - including cases where not every pixel in upstream layer
contributed to every pixel in downstream layer
- test model.train()
- add model.fit()

Change kernel addressing for image gradient computations so that entire subkernel and corresponding error patch
are addressed with slicing for another 25% speed increase. PErformance script now takes 1.5 sec to compute and multilayer
script 4.5 min

Bumped up computations size of performance script so it now takes 4.5sec

Reworked _update_kernel_weights() so that updates for all channels along a single y, x position of a kernel are done
in parallel. Brought some 15% speed increase, performance script now takes 2.7 sec, multilayer 4 mins

Bottlenecks now are:
_get_image_gradients() - 23.4%
_update_kernel_weights() - 9.5%

Updated _get_image_gradients() to compute image gradients across all channels at once. Brought some 30% speed increase,
performance script now takes 1.8sec and multilayer 2 mins 15 sec

Bottlenecks now are:
_get_preactivation() - 13.32%
_update_kernel_weights() - 10.78%
_get_image_gradients() - 9.92%

Reworked _get_preactivation to compute convolution at a single patch over all kernels at once.
This brought some 40% speedup. Performance script now takes 1.2 sec, multilayer takes about 1min 20 sec

Bottlenecks now are:
_get_image_gradients() - 29.11%
_update_kernel_weights() - 19.3%

Performance: 4.1 sec, multilayer 1min 20sec

Reworked _get_image_gradients() to compute gradients on a given pixel for all images at once for about 50% speedup.
Performance script now takes 2.6 sec and multilayer 40 sec


Bottlenecks now are:
- _update_kernels() - 24.34%
_get_image_gradients() - 9.98%
