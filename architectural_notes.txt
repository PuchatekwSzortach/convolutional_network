

Model.fit:

    Shuffle data
    For each batch:

       - For each layer from start to end:

            Call train_forward() - does computations as in forward, but caches inputs and outputs

        - For each layer from end to start:

            Call train_backward() - performs backpropagation and applies results


Ok, what kind of tests could I write? What would I want to test?

Input layer - that shape is legal, that when compiling it sets output shape correctly and that during forward and train_forward
it checks for correct input and sets output

Flatten layer - that sets output_shape correctly at construction, that at compile accepts only shapes that can be squeezed to 2D,
    that during forward and train_forward outputs correct stuff

Softmax - that checks for legal input shape, that computes correct output, that computes correct loss,
    that compute correct loss derivative, that updates itself correctly, that passes correct loss to previous layer

Convolution2D - that creates correct kernels,
    that computes correct convolutions, that computes correct derivatives, that updates itself correctly


Ok, what do we want to test with Model:
- test model calls proper functions for fit

Ok, now how will model.fit(x) work?

    Shuffle data

    For batch in data:

        x = batch

        for layer in self.layers:

            x = layer.train_forward(x)

        gradients = 1

        for layer in reversed(self.layers):

            gradients = layer.train_backward(gradients, learning_rate)

I guess :P

Ok, so what components are needed for backpropagation?

Our fit will have fit_batch().
fit_batch() would do:
- model would calculate loss
- pass that loss to softmax
- softmax would calculate gradiens to its preactivation
- flatten would calculate its gradients
- input would have a no op there I guess
- convolution 2D would do its gradients

start with model.get_loss()


Hmm, so I know how to compute error gradients across Softmax layer.
But - to do that it needs:
- predictions
- correct labels
Predictions it can cache in train_forward(), but what about correct labels? How do they get in there?
I need train_backward(correct_labels)

Convolution backpropagation:


Not yet checked cases:

Ok, tests we need for computing (and passing!) dC/da(l-1):
- all the existing tests - we can add gradients checks to them
- a larger case - say 3x3 kernel and 4x4 result, so 6x6 previous layer... I guess



Further tests needed:
- pass gradients from convolutional layer to upstream layer - including cases where not every pixel in upstream layer
contributed to every pixel in downstream layer
- test model.train()
- add model.fit()

Performance at 100 batches:
Single layer: 0.54 after first epoch, 0.73 after third epoch

Ok, now try to work on speeding our convolutional network up a bit.

Current implementation: 6 seconds

Main bottlenecks:
- _get_image_gradients
- _update_kernel_weights

Added a change to compute image gradients over all input channels at once for a 25% speedup - now 4~4.5 sec

Now _update_kernel_weights is the main bottleneck

Changed _update_kernel_weights to compute kernel weight updates using error derivatives for all images in a vectorized fashion,
cutting down computational time by another 25% - now performance script takes 3 seconds


