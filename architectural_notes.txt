
Ok, think this through.

We will have layers:
Input, Convolution2D, Flatten, Softmax

And model class.

We will make model with:
Model(layers)

It will check that it starts with Input and finished with Softmax

Softmax will make checks, etc, etc.

Model will have api:
- constructor
- fit
- predict

Model.constructor:

    - model checks first layer is input and last is softmax

    Then starting from first to last:
    - for Input - set output_shape
    - for Convolution2D:
        - get depth of its predecessor
        - initialize kernels accordingly
        - set its own output_shape
    - for Flatten - call np.flat(), make sure output is 2D, set output_shape
    - Softmax - make sure output is 2D and labels dimension is 2 or more

Model.predict:

    1. For each layer call forward
        - Input.forward(x) - checks shapes correctness
        - Convolution2D.forward(x) - gets data, convolves, adds biase, relu nonlinearity, outputs result
        - Flatten.forward(x) - calls np.flat, makes sure output is 2D, outputs result
        - Softmax - computes softmax, outputs result - makes sure input to softmax has expected shape
    2. Output result

Model.fit:

    Shuffle data
    For each batch:

       - For each layer from start to end:

            Call train_forward() - does computations as in forward, but caches inputs and outputs

        - For each layer from end to start:

            Call train_backward() - performs backpropagation and applies results


Ok, what kind of tests could I write? What would I want to test?

Input layer - that shape is legal, that when compiling it sets output shape correctly and that during forward and train_forward
it checks for correct input and sets output

Flatten layer - that sets output_shape correctly at construction, that at compile accepts only shapes that can be squeezed to 2D,
    that during forward and train_forward outputs correct stuff

Softmax - that checks for legal input shape, that computes correct output, that computes correct loss,
    that compute correct loss derivative, that updates itself correctly, that passes correct loss to previous layer

Convolution2D - that creates correct kernels,
    that computes correct convolutions, that computes correct derivatives, that updates itself correctly


Ok, so maybe I shouldn't really be doing any real work in constructors and everything should be done in compile?

Well, think through carefully what could be done in constructor vs what needs to be done in compile step:

Input:
    - constructor - check input is 3D and integer, set output shape
    - build - nothing really

Flatten:
    - constructor - nothing really
    - build - check previous layer shape can be squeezed properly, set output shape

Softmax:
    - constructor - nothing really
    - build - check previous layer shape is legal, set output shape

Convolution2D:
    - constructor - nothing really
    - build - check previous layer shape and initialize kernels accordingly, set output shape

Ok, what do we want to test with Model:
- test model builds for valid input and doesn't for invalid input
- test model outputs proper values for predict
- test model calls proper functions for fit


Now think through how will the forward loop exactly look. Let's put model in charge of pushing computations between layers:

So model.predict(x) will do:

    for layer in self.layers:

        x = layer.forward(x)

    return x

Ok, now how will model.fit(x) work?

    Shuffle data

    For batch in data:

        x = batch

        for layer in self.layers:

            x = layer.train_forward(x)

        gradients = 1

        for layer in reversed(self.layers):

            gradients = layer.train_backward(gradients, learning_rate)

I guess :P


Ok, think through the input and output shapes.
Three issues:
1 - should they include batch size or not
2 - using of None - when would it be needed for different types of layers
3 - should we use tuple or list for shapes?


1. For input layer etc, makes no real difference. Should make no difference elsewhere. Using sample_shape we can store whole shape.
Is there any real advantage to it? Well, same output dimensionality in error messages.

2. When would we use None in input shapes?
- batch dimension
- convolutional layers - I guess I would use a different error message that explains problem better
So only in batch dimension

3. Should we use tuple or list for shapes?
Tuple, that's what numpy uses.

4. So how do we allow lists as inputs? Just cast them to tuples if inputs are lists.


Ok, what check should flatten layer build() make? None I think

Now should Flatten.forward() perform a check? No, I think not