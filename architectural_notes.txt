

- for Convolution2D:
    - get depth of its predecessor
    - initialize kernels accordingly
    - set its own output_shape

Model.fit:

    Shuffle data
    For each batch:

       - For each layer from start to end:

            Call train_forward() - does computations as in forward, but caches inputs and outputs

        - For each layer from end to start:

            Call train_backward() - performs backpropagation and applies results


Ok, what kind of tests could I write? What would I want to test?

Input layer - that shape is legal, that when compiling it sets output shape correctly and that during forward and train_forward
it checks for correct input and sets output

Flatten layer - that sets output_shape correctly at construction, that at compile accepts only shapes that can be squeezed to 2D,
    that during forward and train_forward outputs correct stuff

Softmax - that checks for legal input shape, that computes correct output, that computes correct loss,
    that compute correct loss derivative, that updates itself correctly, that passes correct loss to previous layer

Convolution2D - that creates correct kernels,
    that computes correct convolutions, that computes correct derivatives, that updates itself correctly


Ok, so maybe I shouldn't really be doing any real work in constructors and everything should be done in compile?

Well, think through carefully what could be done in constructor vs what needs to be done in compile step:

Convolution2D:
    - constructor - accepts arguments: nb_filter, nb_row, nb_col. For now we don't allow step size and use only valid steps.
    - build - check previous layer shape and initialize kernels accordingly, set output shape

Ok, what do we want to test with Model:
- test model builds for valid input and doesn't for invalid input
- test model outputs proper values for predict
- test model calls proper functions for fit

Ok, now how will model.fit(x) work?

    Shuffle data

    For batch in data:

        x = batch

        for layer in self.layers:

            x = layer.train_forward(x)

        gradients = 1

        for layer in reversed(self.layers):

            gradients = layer.train_backward(gradients, learning_rate)

I guess :P

Ok, now when building convolutional layers:
- I should make sure input is valid
What constitutes a valid input?
- 4D!
- what else? Well, I guess that's it for now
